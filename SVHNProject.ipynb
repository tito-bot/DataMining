{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19NhFm5RXuI273qrPqRj-dLux2_KNdETd",
      "authorship_tag": "ABX9TyNMin7hG7ge2px9E/a9I8n8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tito-bot/DataMining/blob/main/SVHNProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU0CsaKYojJ6",
        "outputId": "6982a709-b109-4b66-e520-054e3ae0fa08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# prompt: write code coonect google drivve       to google colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AMlBuwbHqttL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1keHPWUqsmX",
        "outputId": "454345ee-f144-41ac-b26f-ae82c98e7255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\" Helper function for loading a MATLAB v7.3 HDF5 File \"\"\"\n",
        "    with h5py.File(path, 'x') as f:\n",
        "        X = np.array(f['X'])  # Adjust the dataset name if it's different\n",
        "        y = np.array(f['y'])\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "05vGg0o2sHd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install h5py mat73\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define dataset paths\n",
        "train_dataset_path = '/content/drive/MyDrive/Colab/train_digitStruct.mat'\n",
        "test_dataset_path = '/content/drive/MyDrive/Colab/test_digitStruct.mat'\n",
        "\n",
        "# Load data function\n",
        "def load_data(path):\n",
        "    \"\"\" Helper function for loading a MATLAB v7.3 HDF5 File \"\"\"\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        # Print the available keys to identify the correct dataset names\n",
        "        print(f.keys())\n",
        "        X = np.array(f['#refs#'])  # Replace '#refs#' with the correct dataset key if needed\n",
        "        y = np.array(f['digitStruct'])  # Replace 'digitStruct' with the correct dataset key if needed\n",
        "    return X, y\n",
        "\n",
        "# Load training and test data\n",
        "X_train, y_train = load_data(train_dataset_path)\n",
        "X_test, y_test = load_data(test_dataset_path)\n",
        "\n",
        "# Transpose the dataset: (width, height, channels, samples) -> (samples, width, height, channels)\n",
        "# X_train = np.transpose(X_train, (3, 0, 1, 2))\n",
        "# X_test = np.transpose(X_test, (3, 0, 1, 2))\n",
        "\n",
        "# Normalize the image data (from 0-255 to 0-1)\n",
        "# X_train = X_train.astype('float32')/255.0\n",
        "# X_test = X_test.astype('float32')/255.0\n",
        "\n",
        "# Handle label 10 (which represents '0') as 0 for classification\n",
        "# y_train[y_train == 10] = 0\n",
        "# y_test[y_test == 10] = 0\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "# y_train = to_categorical(y_train)\n",
        "# y_test = to_categorical(y_test)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "### Building and Training the CNN Model (unchanged from your code)\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Build the model\n",
        "# model = build_model()\n",
        "\n",
        "# Train the model\n",
        "# history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "# test_loss, test_acc = model.evaluate(X_test, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hf5e45nasLnu",
        "outputId": "db1434e1-01d7-4a1f-9c9d-0ef621019931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: mat73 in /usr/local/lib/python3.10/dist-packages (0.65)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.26.4)\n",
            "<KeysViewHDF5 ['#refs#', 'digitStruct']>\n",
            "<KeysViewHDF5 ['#refs#', 'digitStruct']>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install h5py\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define dataset paths\n",
        "train_dataset_path = '/content/drive/MyDrive/Colab/train_digitStruct.mat'\n",
        "test_dataset_path = '/content/drive/MyDrive/Colab/test_digitStruct.mat'\n",
        "\n",
        "# Function to load data from the .mat file (HDF5 format)\n",
        "def load_data(path):\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        X = np.array(f['X'])  # Change 'X' to the correct key name based on dataset\n",
        "        y = np.array(f['y'])  # Change 'y' to the correct key name based on dataset\n",
        "    return X, y\n",
        "\n",
        "# Explore the .mat file to check the variable names\n",
        "def explore_mat_file(path):\n",
        "    with h5py.File(path, 'r') as f:\n",
        "        print(\"Keys in the MAT file:\", list(f.keys()))\n",
        "        for key in f.keys():\n",
        "            if isinstance(f[key], h5py.Dataset): # Check if the object is a Dataset\n",
        "                print(f\"Shape of {key}:\", f[key].shape)\n",
        "            else:\n",
        "                print(f\"{key} is a Group\")\n",
        "\n",
        "# Explore the training dataset to get variable names\n",
        "explore_mat_file(train_dataset_path)\n",
        "\n",
        "# Load the training and test data\n",
        "X_train, y_train = load_data(train_dataset_path)\n",
        "X_test, y_test = load_data(test_dataset_path)\n",
        "\n",
        "# Transpose the dataset: (width, height, channels, samples) -> (samples, width, height, channels)\n",
        "X_train = np.transpose(X_train, (3, 0, 1, 2))\n",
        "X_test = np.transpose(X_test, (3, 0, 1, 2))\n",
        "\n",
        "# Normalize the image data (from 0-255 to 0-1)\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Handle label 10 (which represents '0') as 0 for classification\n",
        "y_train[y_train == 10] = 0\n",
        "y_test[y_test == 10] = 0\n",
        "\n",
        "# Convert the labels to one-hot encoding\n",
        "y_train = to_categorical(y_train, 10)  # 10 classes for digits 0-9\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Training Set\", X_train.shape, y_train.shape)\n",
        "print(\"Validation Set\", X_val.shape, y_val.shape)\n",
        "print(\"Test Set\", X_test.shape, y_test.shape)\n",
        "\n",
        "### Building the CNN Model\n",
        "def build_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolution layer\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Second convolution layer\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Third convolution layer\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    # Flatten the output\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Output layer (10 digits, softmax activation for classification)\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Build the CNN model\n",
        "model = build_model()\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "### Training the Model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_val, y_val))\n",
        "\n",
        "### Evaluating the Model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f'Test accuracy: {test_acc * 100:.2f}%')\n",
        "\n",
        "### Plotting Training History\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Accuracy Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Loss Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "RLyRT1rjuD3V",
        "outputId": "8bbb9613-2ae2-4d6d-9925-98222f3fc62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.26.4)\n",
            "Keys in the MAT file: ['#refs#', 'digitStruct']\n",
            "#refs# is a Group\n",
            "digitStruct is a Group\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Unable to synchronously open object (object 'X' doesn't exist)\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6b8ebc26e7df>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Load the training and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-6b8ebc26e7df>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Change 'X' to the correct key name based on dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Change 'y' to the correct key name based on dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             raise TypeError(\"Accessing a group is done with bytes or str, \"\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'X' doesn't exist)\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TvvQxQR4uOdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}